---
title: "AWS"
---

import UpstashRedis from '/snippets/upstash-redis-aws-eks.mdx';

It's a step-by-step Ship deployment guide. We will use [Amazon Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks), [Mongo Atlas](https://www.mongodb.com/), [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr), [GitHub Actions](https://github.com/features/actions) for automated deployment, and [CloudFlare](https://www.cloudflare.com) for DNS and SSL configuration.

You need to create [GitHub](https://github.com), [AWS](https://aws.amazon.com), [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/register) and [CloudFlare](https://www.cloudflare.com/) accounts and install the next tools on your machine before starting:

* [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) - CLI tool for accessing Kubernetes cluster;
* [kubectx](https://github.com/ahmetb/kubectx) - CLI tool for easier switching between Kubernetes contexts;
* [helm](https://helm.sh/docs/intro/install) - CLI tool for managing Kubernetes deployments;
* [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) - CLI tool for managing AWS resources;
* [eksctl](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html) - CLI tool for managing EKS clusters;
* [k8sec](https://github.com/dtan4/k8sec) - CLI tool for managing Kubernetes Secrets easily;

<Accordion title="How to install k8sec on Linux?">
  Download k8sec tar.gz, then do:

  ```
  chmod +x k8sec
  ```
  ```
  sudo cp k8sec /usr/local/bin/
  ```
  ```
  k8sec --help
  ```
</Accordion>

Try the next commands to ensure that everything is installed correctly:

```
kubectl

kubectx

helm

aws sts get-caller-identity

eksctl

k8sec
```

Also, you need [git](https://git-scm.com/) and [Node.js](https://nodejs.org/en/) if you already haven't.

## Setup project

First, initialize your project. Type ```npx create-ship-app@latest``` in the terminal then choose **AWS EKS** deployment type.

![Init project](/images/aws/init-project-aws-eks.png)

You will have the next project structure.

```shell
/my-ship-app
  /.github
  /apps
    /api
    /web
  /deploy
  ...
```

## AWS Regions

AWS Regions are physical locations of AWS cluster data centers. Each group of logical data centers calls Availability Zone (AZ). AZs allow the operation of production applications and databases that are more highly available, fault-tolerant and scalable.

Now you need to select an AWS region for future use of the services. You can read more about region selection for your workloads here: [What to Consider when Selecting a Region for your Workloads](https://aws.amazon.com/blogs/architecture/what-to-consider-when-selecting-a-region-for-your-workloads/).

For this deployment guide, we will use the **us-east-1** region.

<Tip>
Usually, you have to create AWS resources in a single region. If you don't see created resources, you may need to switch to the appropriate AWS region.
</Tip>

## Container registry

You need to create [private repositories](https://console.aws.amazon.com/ecr/private-registry/repositories/create) for storing Docker images. The deployment script will upload images to Container Registry during the build step, and Kubernetes will automatically pull these images from Container Registry to run a new version of the service during the deployment step.

Now we should create a repository for each service.

For Ship, we need to create repositories for the next services:
* [**API**](/api-reference/overview) - api
* [**Migrator**](/migrator) - migrator
* [**Scheduler**](/scheduler) - scheduler
* [**Web**](/web/overview) - web

![Container Registry creation](/images/aws/ecr-creation.png)

<Tip>
You should create a private repository for each service manually.
</Tip>

After creation, you should have the following 4 services in ECR

![Container Registry creation](/images/aws/ecr-created.png)

Docker images for each service are stored in a separate repository.
During the deployment process script will automatically create paths to repositories in the next format:

* [**API**](/api-reference/overview) - 276472736030.dkr.ecr.us-east-1.amazonaws.com/api;
* [**Migrator**](/migrator) - 276472736030.dkr.ecr.us-east-1.amazonaws.com/migrator;
* [**Scheduler**](/scheduler) - 276472736030.dkr.ecr.us-east-1.amazonaws.com/scheduler;
* [**Web**](/web/overview) - 276472736030.dkr.ecr.us-east-1.amazonaws.com/web;

<Tip>
Repository name`276472736030.dkr.ecr.us-east-1.amazonaws.com/api` consists of 5 values:
* `276472736030` - AWS account ID;
* `us-east-1` - AWS region.
* `dkr.ecr` - AWS service.
* `amazonaws.com` - AWS domain.
* `api` - service name.
</Tip>

<Tip>
Images for all environments will be uploaded to the same repository for each service.
</Tip>

## Kubernetes Cluster

Now let's [create EKS cluster](https://console.aws.amazon.com/eks/cluster-create).

<Steps>
  <Step title="Select Custom Configuration">
    Navigate to the cluster creation page and choose `Custom configuration` <br/>
    Make sure to disable `EKS Auto Mode`

    <Frame>
      <img src="/images/aws/cluster-creation-configuration-options.png" alt="Cluster configuration" />
    </Frame>
  </Step>

  <Step title="Name Your Cluster">
    Enter a name for your cluster. It's recommended to use your project name.

    <Frame>
      <img src="/images/aws/cluster-creation-naming.png" alt="Cluster naming" />
    </Frame>

    <Tip>
      For multi-environment setups, append the environment name to your cluster:
      - `my-ship-app-staging`
      - `my-ship-app-production`
    </Tip>
  </Step>

  <Step title="Configure Cluster IAM Role">
    For the `Cluster IAM role`:

    1. Click the `Create recommended role` button
    2. AWS will automatically create IAM roles with necessary EKS cluster permissions
    3. Return to cluster creation page and select the created policy

    <Frame>
      <img src="/images/aws/cluster-creation-cluster-iam-role.png" alt="Cluster IAM role" />
    </Frame>
  </Step>

  <Step title="Set Authentication Mode">
    In the `Cluster access` section:
    - Set `Cluster authentication mode` to `EKS API and ConfigMap`

    <Frame>
      <img src="/images/aws/cluster-creation-access.png" alt="Cluster access" />
    </Frame>
  </Step>

  <Step title="Configure Add-ons">
    Navigate to 'Select add-ons' and verify these required add-ons are selected:
    - CoreDNS
    - kube-proxy
    - Amazon VPC CNI
    - Node monitoring agent

    <Frame>
      <img src="/images/aws/cluster-creation-addons.png" alt="Cluster addons" />
    </Frame>
  </Step>

  <Step title="Review and Create">
    Move to the review section and verify all configuration parameters are correct before creating the cluster.

    <Note>
      Default values for other configuration parameters are suitable unless you have specific requirements.
    </Note>
  </Step>
</Steps>



After creation, you need to wait a few minutes until the cluster status becomes **Active**.

![Cluster Created](/images/aws/cluster-active-state.png)

After cluster creation, you should attach [EC2](https://aws.amazon.com/ec2) instances to the cluster. You can do it by clicking on the **Add Node Group** button on the **Compute** tab.

![Add Node Group](/images/aws/cluster-computing.png)

Set the node group name as `pool-app` and select the relevant Node IAM role from the list.

If you don't have any IAM roles here, click the `Create recommended role` button. You will be prompted to create properly configured IAM roles with all necessary permissions.

![Node Group Configuration](/images/aws/node-group-configuration.png)

AWS recommends creating at least 2 nodes **t3.medium** instance type for the production environment.

![Node Group Instance Configuration](/images/aws/node-group-instance-configuration.png)

<Note>
  Default values for other configuration parameters are suitable unless you have specific requirements.
</Note>

## Accessing a cluster from a local machine

<Warning>
  Before proceeding, ensure you have [configured the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html).
</Warning>

<Steps>
  <Step title="Update kubeconfig">
    Run the following command to configure cluster access:

    ```bash
    aws eks update-kubeconfig \
      --region us-east-1 \
      --name my-ship-app \
      --alias my-ship-app
    ```

    <Note>
      Replace `us-east-1` with your cluster's region and `my-ship-app` with your cluster name.
    </Note>
  </Step>

  <Step title="Switch to cluster context">
    Execute `kubectx` in your terminal and select your cluster from the list.

    ```bash
    kubectx
    ```

    <Frame>
      <img src="/images/aws/kubectx.png" alt="Kubectx selection" />
    </Frame>
  </Step>

  <Step title="Verify cluster access">
    Check the installed pods by running:

    ```bash
    kubectl get pods -A
    ```

    You should see a list of system pods in your cluster:

    <Frame>
      <img src="/images/aws/pods-list.png" alt="List of pods" />
    </Frame>
  </Step>
</Steps>

## Ingress NGINX Controller

[ingress-nginx](https://github.com/kubernetes/ingress-nginx) is an Ingress controller for Kubernetes using [NGINX](https://nginx.org) as a reverse proxy and load balancer.

<Tip>
  Learn more about ingress-nginx functionality in the **[official documentation](https://docs.nginx.com/nginx-ingress-controller/intro/how-nginx-ingress-controller-works/)**.
</Tip>

<Steps>
  <Step title="Navigate to dependencies directory">
    Change to the `deploy/dependencies` directory in your terminal.
  </Step>

  <Step title="Configure Helm Values (Optional)">

    This step is required **only if** you specified a custom node group name in your EKS cluster.

    If you did, update the `eks.amazonaws.com/nodegroup` value in `values.yaml.gotmpl`:

    ```yaml deploy/dependencies/ingress-nginx/values.yaml.gotmpl {5}
    controller:
      publishService:
        enabled: true
      nodeSelector:
        eks.amazonaws.com/nodegroup: pool-app

    rbac:
      create: true

    defaultBackend:
      enabled: false
    ```
  </Step>


  <Step title="Install dependencies">
    Install helm dependencies using helmfile:

    ```bash
    helmfile deps
    ```
  </Step>

  <Step title="Review and apply changes">
    Preview the changes first:

    ```bash
    helmfile diff
    ```

    If the preview looks correct, apply the configuration:

    ```bash
    helmfile apply
    ```
  </Step>
</Steps>

## DNS and SSL

<Steps>
  <Step title="Get Load Balancer Address">
    After deploying ingress-nginx, retrieve the Load Balancer's external hostname:

    ```bash
    kubectl get svc ingress-nginx-controller -n ingress-nginx -o json | jq -r '.status.loadBalancer.ingress[0].hostname'
    ```

    <Note>
      If you have trouble running the above command, you can alternatively use:

      ```bash
      kubectl get svc ingress-nginx-controller -n ingress-nginx
      ```

      And copy the value from the `EXTERNAL-IP` column.
    </Note>
  </Step>

  <Step title="Domain Naming Convention">
    You can follow this recommended naming pattern for different environments:

    | Environment | API Domain           | Web Domain           |
    |-------------|----------------------|----------------------|
    | Production  | api.ship.com         | app.ship.com         |
    | Staging     | api.staging.ship.com | app.staging.ship.com |
  </Step>

  <Step title="Configure DNS in Cloudflare">
    1. First, ensure you have a domain in Cloudflare. You can either:
    - [Register a new domain](https://developers.cloudflare.com/registrar/get-started/register-domain/)
    - [Transfer an existing domain](https://developers.cloudflare.com/registrar/get-started/transfer-domain-to-cloudflare/)

    2. In the Cloudflare DNS tab, create 2 `CNAME` records:
    - One for Web interface
    - One for API endpoint

    Both should point to your Load Balancer's external hostname.

    Enable the **Proxied** option to:
    - Route traffic through Cloudflare
    - Generate SSL certificates automatically

    <Frame>
      <img src="/images/aws/cloudflare-api.png" alt="CloudFlare API DNS Configuration" />
    </Frame>

    <br/>

    <Frame>
      <img src="/images/aws/cloudflare-web.png" alt="CloudFlare Web DNS Configuration" />
    </Frame>

    <Note>
      Cloudflare's free Universal SSL certificates only cover the apex domain and one subdomain level. For multiple subdomain levels, you'll need an [Advanced Certificate](https://developers.cloudflare.com/ssl/edge-certificates/advanced-certificate-manager/manage-certificates/).
    </Note>
  </Step>

  <Step title="Update Configuration Files">
    Update your domain settings in the appropriate environment configuration files:

    For API service:
    <CodeGroup>
      ```yaml deploy/app/api/production.yaml
      service: api
      port: 3001
      domain: api.my-ship-app.paralect.com
      ```

      ```yaml deploy/app/api/staging.yaml
      service: api
      port: 3001
      domain: api.my-ship-app.staging.paralect.com
      ```
    </CodeGroup>

    For Web service:
    <CodeGroup>
      ```yaml deploy/app/web/production.yaml
      service: web
      port: 3002
      domain: my-ship-app.paralect.com
      ```

      ```yaml deploy/app/web/staging.yaml
      service: web
      port: 3002
      domain: my-ship-app.staging.paralect.com
      ```
    </CodeGroup>
  </Step>
</Steps>

## MongoDB Atlas

[MongoDB Atlas](https://cloud.mongodb.com/) is a fully managed cloud database service that provides automated backups, scaling, and security features. It offers 99.995% availability with global deployment options and seamless integration with AWS infrastructure.

### Cluster Creation

<Steps>
  <Step title="Access MongoDB Atlas">
    Sign in to your [MongoDB Atlas account](https://cloud.mongodb.com/) and create a new project if needed.
  </Step>

  <Step title="Deploy New Cluster">
    Click **Create** to start cluster deployment.

    **Cluster Tier Selection:**
    - **Staging**: `M0` (Free tier) - Suitable for development and testing
    - **Production**: `M10` or higher - Includes automated backups and advanced features

    **Provider & Region:**
    - Select **AWS** as your cloud provider
    - Choose the **same region** as your EKS cluster for optimal performance

    <Frame>
      <img src="/images/mongodb-cloud/deploy-cluster.png" alt="Deploy MongoDB Atlas cluster" />
    </Frame>
  </Step>

  <Step title="Configure Cluster Name">
    Enter a descriptive cluster name (e.g., `ship-production-cluster`, `ship-staging-cluster`)
  </Step>
</Steps>

### Security Configuration

<Steps>
  <Step title="Create Database User">
    Navigate to **Database Access** → **Add New Database User**

    - **Authentication Method**: Password
    - **Username**: Use environment-specific names (e.g., `api-production`, `api-staging`)
    - **Password**: Generate a strong password
    - **Database User Privileges**: **Read and write to any database**

    <Frame>
      <img src="/images/mongodb-cloud/add-database-user.png" alt="Add MongoDB database user" />
    </Frame>

    <Warning>
      **Password Requirements**: Ensure the password starts with a letter and contains only alphanumeric characters and common symbols. Special characters at the beginning can cause URI parsing issues.
    </Warning>
  </Step>

  <Step title="Configure Network Access">
    Navigate to **Network Access** → **Add IP Address**

    - Click **Allow access from anywhere** to allow connections from any IP with valid credentials
    - For production, consider restricting to specific IP ranges for enhanced security

    <Frame>
      <img src="/images/mongodb-cloud/network-access.png" alt="Configure MongoDB network access" />
    </Frame>
  </Step>
</Steps>

### Get Connection String

<Steps>
  <Step title="Access Connection Details">
    Go to your cluster dashboard and click the **Connect** button.

    <Frame>
      <img src="/images/mongodb-cloud/dashboard.png" alt="MongoDB Atlas dashboard" />
    </Frame>
  </Step>

  <Step title="Copy Connection String">
    1. Select **Drivers** in the "Connect your application" section
    2. Choose **Node.js** driver and latest version
    3. Copy the connection string and replace `<db_password>` with your actual password

    <Frame>
      <img src="/images/mongodb-cloud/cluster-connect.png" alt="MongoDB connection string" />
    </Frame>

    **Example Connection String:**
    ```bash
    mongodb+srv://api-production:your-password@cluster0.xxxxx.mongodb.net/?retryWrites=true&w=majority
    ```
  </Step>

  <Step title="Save Connection Details">
    Store the connection string securely - you'll need it for environment configuration later
  </Step>
</Steps>

<Tip>
  Before deploying to production, configure [automated backups](https://www.mongodb.com/docs/atlas/backup-restore-cluster/) in the Atlas console to ensure data recovery capabilities.
</Tip>

## Environment variables

Kubernetes applications require proper environment variable configuration for both API and Web components. This section covers how to set up and manage environment variables securely using Kubernetes secrets and configuration files.

### API Environment Variables

For the API deployment, you need to set up environment variables using Kubernetes secrets to securely manage sensitive configuration data.

<Info>
  **Secrets** in Kubernetes are used to store sensitive information, such as passwords, API tokens, and keys.
  They are encoded in Base64 format to provide a level of security.
  These can be mounted into containers as data volumes or used as environment variables.
</Info>

Before deploying the app, make sure all necessary variables from the API config are exist. Here are the minimal set of required variables:

| Name            | Description                | Example value                                      |
|-----------------|----------------------------|----------------------------------------------------|
| `APP_ENV`       | Application environment    | `production`                                       |
| `MONGO_URI`     | Database connection string | `mongodb://<username>:<password>@ship.mongodb.net` |
| `MONGO_DB_NAME` | Database name              | `api-production`                                   |
| `API_URL`       | API domain URL             | `https://api.my-ship-app.paralect.com`             |
| `WEB_URL`       | Web app domain URL         | `https://my-ship-app.paralect.com`                 |

#### Environment Variable Details

<Accordion title="APP_ENV">
  Specifies the application environment (development, staging, production). This controls logging levels, debugging features, error reporting, and other environment-specific behaviors. The API uses this to determine which configuration settings to load.
</Accordion>

<Accordion title="MONGO_URI">
  MongoDB connection string including authentication credentials and cluster information. This is the primary database connection for the API. Format: `mongodb+srv://username:password@cluster.mongodb.net`. Each environment should use a separate database cluster or at minimum separate credentials.
</Accordion>

<Accordion title="MONGO_DB_NAME">
  Name of the MongoDB database to use for this environment. Each environment (development, staging, production) should have its own database to prevent data conflicts and ensure proper isolation.
</Accordion>

<Accordion title="API_URL">
  The fully qualified domain name where the API will be accessible. This must be a valid HTTPS URL and should match your Kubernetes ingress configuration. Used for CORS settings and internal service communication.
</Accordion>

<Accordion title="WEB_URL">
  The fully qualified domain name where the web application will be accessible. Used for CORS configuration, redirect URLs, email templates, and social sharing metadata. Must be a valid HTTPS URL.
</Accordion>

#### Setting up Kubernetes Secrets

<Steps>
  <Step title="Create namespaces and secret objects">
    Create Kubernetes namespaces and secret objects for staging and production environments:

    ```bash
    kubectl create namespace staging
    kubectl create secret generic api-staging-secret -n staging
    kubectl create namespace production
    kubectl create secret generic api-production-secret -n production
    ```
  </Step>

  <Step title="Initialize secret storage">
    First, create an `APP_ENV` variable to initialize secret storage for k8sec:

    <CodeGroup>
      ```bash production
      k8sec set api-production-secret APP_ENV=production -n production
      ```

      ```bash staging
      k8sec set api-staging-secret APP_ENV=staging -n staging
      ```
    </CodeGroup>
  </Step>

  <Step title="Verify secret creation">
    Run the following command to check the created secret:

    <CodeGroup>
      ```bash production
      k8sec list api-production-secret -n production
      ```

      ```bash staging
      k8sec list api-staging-secret -n staging
      ```
    </CodeGroup>
  </Step>

  <Step title="Prepare environment file">
    Create a `.env.production` file with all required variables:

    <CodeGroup>
      ```bash .env.production
      APP_ENV=production
      MONGO_URI=mongodb://username:password@ship.mongodb.net
      MONGO_DB_NAME=api-production
      API_URL=https://api.my-ship-app.paralect.com
      WEB_URL=https://my-ship-app.paralect.com
      ```

      ```bash .env.staging
      APP_ENV=staging
      MONGO_URI=mongodb://username:password@ship.mongodb.net
      MONGO_DB_NAME=api-staging
      API_URL=https://api.my-ship-app.staging.paralect.com
      WEB_URL=https://my-ship-app.staging.paralect.com
      ```
    </CodeGroup>


    <Warning>
      Replace all example values with your actual configuration. Never use production secrets in documentation or version control.
    </Warning>
  </Step>

  <Step title="Import secrets to Kubernetes">
    Import secrets from the .env file to Kubernetes secret using k8sec:

    <CodeGroup>
      ```bash production
      k8sec load -f .env.production api-production-secret -n production
      ```

      ```bash staging
      k8sec load -f .env.staging api-staging-secret -n staging
      ```
    </CodeGroup>
  </Step>
</Steps>

<Warning>
  After updating environment variables, you must initiate a new deployment for changes to take effect.
  Kubernetes pods cache variable values during startup, requiring a pod restart or rolling update to apply changes.
</Warning>

### Web Environment Variables

The web application uses Next.js environment variables that are embedded at build time and made available in the browser. Unlike API secrets, these variables are stored directly in the GitHub repository.

<Info>
  **Why Web Environment Variables Are Safe in Git**: Web environment variables (prefixed with `NEXT_PUBLIC_`) contain only public configuration like URLs and API endpoints. They don't include sensitive data like passwords or API keys, making them safe to store in version control. These values are already exposed to users in the browser, so repository storage doesn't create additional security risks.
</Info>

<Warning>
  **Security Notice**: Never store sensitive information (passwords, API keys, secrets) in web environment files as they will be accessible on the client side. Only use public configuration values that are safe to expose to end users.
</Warning>

#### Configuration Files

Web environment variables are stored in separate files for each deployment environment:

<CodeGroup>
  ```bash apps/web/.env.production
  NEXT_PUBLIC_API_URL=https://api.my-ship-app.paralect.com
  NEXT_PUBLIC_WS_URL=https://api.my-ship-app.paralect.com
  NEXT_PUBLIC_WEB_URL=https://my-ship-app.paralect.com
  ```

  ```bash apps/web/.env.staging
  NEXT_PUBLIC_API_URL=https://api.my-ship-app.staging.paralect.com
  NEXT_PUBLIC_WS_URL=https://api.my-ship-app.staging.paralect.com
  NEXT_PUBLIC_WEB_URL=https://my-ship-app.staging.paralect.com
  ```
</CodeGroup>

#### Environment Variables Reference

| Variable              | Description                           | Example                                |
|-----------------------|---------------------------------------|----------------------------------------|
| `NEXT_PUBLIC_API_URL` | Base URL for API requests             | `https://api.my-ship-app.paralect.com` |
| `NEXT_PUBLIC_WS_URL`  | WebSocket server URL for real-time    | `https://api.my-ship-app.paralect.com` |
| `NEXT_PUBLIC_WEB_URL` | App's own URL for redirects/metadata  | `https://my-ship-app.paralect.com`     |

<Tip>
  **Best Practice**: Keep web environment files in your repository and ensure all values are non-sensitive. If you need to reference sensitive data from the frontend, create a secure API endpoint that returns the necessary information after proper authentication.
</Tip>


# Setting up GitHub Actions CI/CD

### Creating IAM user in AWS

To set up CI/CD with GitHub Actions securely, we need to create a dedicated IAM user in AWS with specific permissions.

This separate user will be used exclusively for CI/CD operations, following the principle of least privilege and keeping deployment credentials isolated from other system users.

<Steps>
  <Step title="Create IAM Policy">
    1. Go to [AWS IAM Policies](https://console.aws.amazon.com/iam/home#/policies)
    2. Click **Create policy**
    3. Select **JSON** tab and add the policy:

    ```json
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "ECR",
                "Effect": "Allow",
                "Action": [
                "ecr:BatchGetImage",
                "ecr:CompleteLayerUpload",
                "ecr:GetAuthorizationToken",
                "ecr:UploadLayerPart",
                "ecr:InitiateLayerUpload",
                "ecr:BatchCheckLayerAvailability",
                "ecr:PutImage"
              ],
               "Resource": "*"
            },
            {
                "Sid": "EKS",
                "Effect": "Allow",
                "Action": "eks:DescribeCluster",
                "Resource": "*"
            }
        ]
    }
    ```

    <Frame>
      <img src="/images/aws/policy-config.png" alt="Policy Configuration" />
    </Frame>

    4. (Optional) Add tags
    5. Give the policy a name (e.g. `GitHubActionsDeployPolicy`) and create it

    <Frame>
      <img src="/images/aws/policy-review-create.png" alt="Policy review create" />
    </Frame>
  </Step>

  <Step title="Create IAM User">
    1. Navigate to **Users** in IAM console
    2. Click **Create user**
    3. Give the user a name (e.g. `github-actions`)

    <Frame>
      <img src="/images/aws/user-creating.png" alt="User creating" />
    </Frame>

    4. Attach the policy you created by selecting:
    - **Attach existing policies directly**
    - Choose the CI/CD policy created in previous step

    <Frame>
      <img src="/images/aws/user-policy.png" alt="User policy" />
    </Frame>

    4. (Optional) Add user tags
    5. Review and create user
  </Step>

  <Step title="Generate Access Keys">
    1. Find your new user in the users list and open user's page
    2. Click **Create access key**

    <Frame>
      <img src="/images/aws/user-create-access-key.png" alt="User create access key" />
    </Frame>

    3. Select use case: **Third-party service**

    <Frame>
      <img src="/images/aws/user-access-key-use-case.png" alt="User access key use cases" />
    </Frame>

    4. Save the Access Key ID and Secret Access Key securely

    <Frame>
      <img src="/images/aws/user-credentials.png" alt="User credentials" />
    </Frame>

    <Warning>
      The Secret Access Key will only be shown once - make sure to save it immediately!
    </Warning>
  </Step>

  <Step title="Configure EKS Access">
    1. Copy your user's ARN from the IAM dashboard

    <Frame>
      <img src="/images/aws/user-arn.png" alt="User ARN" />
    </Frame>

    2. Run the following command to grant Kubernetes access:

    ```bash
    eksctl create iamidentitymapping \
    --cluster my-ship-app \
    --group system:masters \
    --username github-actions \
    --arn YOUR_USER_ARN
    ```

    Replace `YOUR_USER_ARN` with the actual ARN copied earlier.
  </Step>
</Steps>

<Note>
  These permissions enable CI/CD workflows while following security best practices:
  - Minimal required permissions for ECR operations
  - Limited EKS access for cluster management
  - Dedicated CI/CD user separate from other IAM users
</Note>

### Configuring GitHub Actions secrets and variables

<Note>
  Before starting, make sure you have created a GitHub repository for your project.
</Note>

GitHub Secrets and variables allow you to manage reusable configuration data.

Secrets are encrypted and are used for sensitive data. [Learn more about encrypted secrets](https://docs.github.com/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets).

Variables are shown as plain text and are used for non-sensitive data. [Learn more about variables](https://docs.github.com/actions/learn-github-actions/variables).

<Tip>
The deployment will be triggered on each commit:
- Commits to **main** branch → deploy to **staging** environment
- Commits to **production** branch → deploy to **production** environment
</Tip>

[Configure](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository) the following secrets and variables in your GitHub repository:

| Name                    | Type     | Description                                                                                                                 |
|-------------------------|----------|-----------------------------------------------------------------------------------------------------------------------------|
| AWS_SECRET_ACCESS_KEY   | secret   | The secret access key from the AWS IAM user created for CI/CD. This allows GitHub Actions to authenticate with AWS services |
| AWS_ACCESS_KEY_ID       | variable | The access key ID from the AWS IAM user. Used in conjunction with the secret key for AWS authentication                     |
| AWS_REGION              | variable | The AWS region where your EKS cluster and ECR registry are located (e.g. `us-east-1`)                                       |
| CLUSTER_NODE_GROUP      | variable | The name of the EKS node group where your application pods will be scheduled (e.g. `pool-app`)                              |
| CLUSTER_NAME_PRODUCTION | variable | The name of your production EKS cluster. Required when deploying to the production environment                              |
| CLUSTER_NAME_STAGING    | variable | The name of your staging EKS cluster. Required when deploying to the staging environment                                    |

<Warning>
Never commit sensitive credentials directly to your repository. <br/>
  Always use GitHub Secrets for sensitive information like AWS keys.
</Warning>

<Note>
Variables (unlike secrets) are visible in logs and can be used for non-sensitive configuration values that may need to be referenced or modified.
</Note>


<Frame>
  <img src="/images/aws/gh-secrets.png" alt="GitHub Secrets" />
</Frame>

<Frame>
  <img src="/images/aws/gh-variables.png" alt="GitHub Variables" />
</Frame>


Now commit all changes to GitHub that will trigger deployment, or you can [run a workflow manually](https://docs.github.com/en/actions/managing-workflow-runs-and-deployments/managing-workflow-runs/manually-running-a-workflow)

<Frame>
  <img src="/images/aws/ci-start.png" alt="CI start" />
</Frame>

Done! Application deployed and can be accessed by the provided domain.


<Frame>
  <img src="/images/aws/ci-finish.png" alt="CI finish" />
</Frame>


<Frame>
  <img src="/images/aws/deployment-finish.png" alt="Deployment finish" />
</Frame>


<Frame>
  <img src="/images/aws/deployed-pods.png" alt="Deployed pods" />
</Frame>

<Tip>
If something went wrong you can check the workflows logs on GitHub and use [**kubectl logs**](https://kubernetes.io/docs/reference/kubectl/cheatsheet/#interacting-with-running-pods), [**kubectl describe**](https://kubernetes.io/docs/reference/kubectl/cheatsheet/#viewing-finding-resources) commands.
</Tip>


## Setting up Upstash Redis database (recommended)
<UpstashRedis/>

